{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0cd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import requests\n",
    "import inflection\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from email import encoders\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "class BookWebScraping( object ):\n",
    "    '''\n",
    "    __(init)__\n",
    "        - Set sender variables and requests.\n",
    "    api_request\n",
    "        - Start request at books.toscrape.\n",
    "    first_scrapy\n",
    "        - Collect showroom data at books.toscrape.\n",
    "    get_links\n",
    "        - Collect all products links.\n",
    "    second_scrapy\n",
    "        - With books links collect more data of books.\n",
    "    save_scrapy\n",
    "        - Save scrapy on a pandas dataframe.\n",
    "    send_email\n",
    "        - Send email to selected user.\n",
    "    '''\n",
    "    def __init__( self ):\n",
    "        self.url = 'https://books.toscrape.com'\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5),AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        self.sender_email = 'f0438iuj9r354hyu9f93@gmail.com'\n",
    "        self.sender_pass  = '49UFH348Y8ygd7eq2gduy#'\n",
    "        \n",
    "    def api_request( self, url, headers ):\n",
    "        page = requests.get( self.url, headers=self.headers )\n",
    "        soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "        page_results = list( filter( None, soup.find( 'form', class_='form-horizontal' ).get_text().split('\\n') ) )[0]\n",
    "        total_products = int( page_results[0:4] )\n",
    "        total_showcase = int( page_results[-3:-1] )\n",
    "        total_requests = int( total_products / total_showcase)\n",
    "\n",
    "        return total_requests\n",
    "\n",
    "    def first_scrapy( self, total_requests ):\n",
    "        aux_p = []\n",
    "        aux_a = []\n",
    "\n",
    "        for i in range( 1, total_requests+1 ): # Get all Price & Name Books.\n",
    "            url_request = 'https://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "\n",
    "            page = requests.get( url_request, headers=self.headers )\n",
    "            soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "            product_showcase = soup.find( 'ol', class_='row' )\n",
    "\n",
    "            #book_name\n",
    "            product_list = product_showcase.find_all('a', title=True)\n",
    "            p_name = [p['title'] for p in product_list]\n",
    "            aux_a.append( p_name )\n",
    "\n",
    "            #book_price\n",
    "            product_list = product_showcase.find_all( 'article', class_='product_pod' )\n",
    "            product_list[1].find('p', class_='price_color').get_text()\n",
    "            p_price = [p.find('p', class_='price_color').get_text().replace('Â£', '') for p in product_list]\n",
    "            aux_p.append( p_price )\n",
    "\n",
    "        p_price = []   # Array with all prices.\n",
    "        for i in aux_p:\n",
    "            for j in i:\n",
    "                p_price.append( j )\n",
    "\n",
    "        p_name = []\n",
    "        for i in aux_a:\n",
    "            for j in i:\n",
    "                p_name.append( j )\n",
    "\n",
    "        df_showcase = pd.DataFrame( [p_name, p_price] ).T\n",
    "        df_showcase.columns = ['name', 'price']\n",
    "\n",
    "        return df_showcase\n",
    "\n",
    "    def get_links( self, total_requests ):\n",
    "        aux_link = []\n",
    "        for i in range( 1, total_requests+1 ): # Get all link info\n",
    "            url_request = 'https://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "\n",
    "            page = requests.get( url_request, headers=self.headers )\n",
    "            soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "            product_showcase = soup.find( 'ol', class_='row' )\n",
    "            product_list = product_showcase.find_all('a', href=True)\n",
    "\n",
    "            for i in range( 1, 40, 2 ):\n",
    "                aux_link.append( product_list[i]['href'] )\n",
    "\n",
    "        return aux_link \n",
    "\n",
    "\n",
    "    def second_scrapy( self, aux_link ):\n",
    "        cols = ['upc', 'category', 'stock', 'price']\n",
    "        df_details = pd.DataFrame()\n",
    "\n",
    "        for i in aux_link:\n",
    "            url  = 'https://books.toscrape.com/catalogue/' + i\n",
    "            page = requests.get( url, headers=self.headers )\n",
    "            soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "            #book_name\n",
    "            p_name = soup.find('h1').get_text()\n",
    "\n",
    "            #book_stock\n",
    "            p_stock = int(list( filter( None, soup.find('p', 'instock availability').get_text().split('\\n') ) )[1].replace(' ', '').replace('Instock(', '').replace('available)', ''))\n",
    "\n",
    "            #book category\n",
    "            p_category = list( filter( None, soup.find('ul', class_='breadcrumb' ).get_text().split('\\n') ) )[2]\n",
    "\n",
    "            #book_id\n",
    "            product_table = soup.find('table', class_='table table-striped')\n",
    "            p_upc = product_table.find('td').get_text()\n",
    "\n",
    "            df_info = pd.DataFrame( [p_name, p_upc, p_category, p_stock] ).T\n",
    "            df_info.columns = ['name', 'upc', 'category', 'stock']\n",
    "\n",
    "            df_details = pd.concat( [df_details, df_info], axis=0 )\n",
    "\n",
    "            return df_details\n",
    "\n",
    "    def save_scrapy( self, df_showcase, df_details ):\n",
    "        df_raw = pd.merge( df_showcase, df_details, on='name', how='left' ) # Join Dataframe\n",
    "        df_raw['scrapy_datetime'] = datetime.now().strftime( '%Y-%m-%d %H:%M:%S' ) # Save Scrapy datetime\n",
    "        df_raw.to_csv( 'books.csv' ) # Save to CSV\n",
    "\n",
    "        return df_raw\n",
    "\n",
    "    def send_email( self, sender_email, sender_pass, receiver_email, df_raw ):\n",
    "        email = '''Olá,\n",
    "        Segue abaixo um anexo com a extração de dados\n",
    "        do site books.toscrape.com as ''' + str(df_raw['scrapy_datetime'][0])\n",
    "\n",
    "        sender_email = self.sender_email\n",
    "        sender_pass  = self.sender_pass\n",
    "        receiver_email = receiver_email\n",
    "\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To']   = receiver_email\n",
    "        msg['Subject'] = 'Coleta de dados de Livros'\n",
    "\n",
    "        msg.attach( MIMEText(email, 'plain') )\n",
    "        attach_file = open( 'books.csv', 'rb' )\n",
    "\n",
    "        pl = MIMEBase( 'application', 'octate-stream' )\n",
    "        pl.set_payload( ( attach_file ).read() )\n",
    "        encoders.encode_base64( pl )\n",
    "        pl.add_header( 'Content-Disposition', \"attachment; filename=books.csv\" )\n",
    "        msg.attach( pl )\n",
    "\n",
    "        session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port\n",
    "        session.starttls() #enable security\n",
    "        session.login( self.sender_email, sender_pass) #login with mail_id and password\n",
    "        text = msg.as_string()\n",
    "        session.sendmail( sender_email, receiver_email, text)\n",
    "        session.quit()\n",
    "\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bws = BookWebScraping()\n",
    "\n",
    "    sender_email = bws.sender_email\n",
    "    sender_pass  = bws.sender_pass\n",
    "    receiver_email = ''\n",
    "    \n",
    "    total_requests = bws.api_request( url, headers )\n",
    "    aux_link = bws.get_links( total_requests )\n",
    "    \n",
    "    df_showcase = bws.first_scrapy( total_requests )\n",
    "    df_details  = bws.second_scrapy( aux_link )\n",
    "    df_raw = bws.save_scrapy( df_showcase, df_details )\n",
    "    \n",
    "    bws.send_email( sender_email, sender_pass, receiver_email, df_raw )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
